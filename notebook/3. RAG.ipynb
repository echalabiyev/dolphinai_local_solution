{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/dolphinai-project/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/azureuser/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!! SOOOOOOOOOOOOOOOOOOOOOOOOOOS !!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# hide the token fom the huggingface\n",
    "login(token=\"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\")\n",
    "#######################################\n",
    "\n",
    "\n",
    "# Define Embedding model\n",
    "# bge_m3_ef =OllamaEmbeddings(model=\"bge-m3\",show_progress=True)\n",
    "bge_m3_ef =OllamaEmbeddings(model=\"bge-m3\")\n",
    "model_id = \"mistralai/Mixtral-8x7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Define LLM\n",
    "llm = Ollama(model=\"dolphinai-mixtral:8x7b\", request_timeout=200.0)\n",
    "\n",
    "# create the length function\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\"\n",
    "    )[\"input_ids\"][0]\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_milvus import Milvus\n",
    "\n",
    "uri=\"http://localhost:19530/dolphinai_db\"\n",
    "vector_store_loaded = Milvus(\n",
    "    bge_m3_ef,\n",
    "    connection_args={\"uri\": uri},\n",
    "    collection_name=\"dolphinai_sap_collection\",\n",
    "    # collection_name=\"dolphinai_collection\",\n",
    "    vector_field=\"embedding\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question =\"trovami gli ordini di vendita di ACME del 2023 contenenti i prodotti 'levigatrice' o 'sega circolare'\"\n",
    "question =\"How does availability control work in SAP?\"\n",
    "# question =\"Come funziona in controllo di disponibilit√† su SAP?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vector_store_loaded.similarity_search(\n",
    "    question,\n",
    "    k=10,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Group by file_name\n",
    "grouped_documents = defaultdict(list)\n",
    "for doc in results:\n",
    "    file_name = doc.metadata['file_name']\n",
    "    grouped_documents[file_name].append(doc)\n",
    "\n",
    "# Sort each group by page_label\n",
    "for file_name, d in grouped_documents.items():\n",
    "    grouped_documents[file_name] = sorted(d, key=lambda x: x.metadata['page_label'])\n",
    "\n",
    "# Convert to a sorted list of (file_name, documents) tuples\n",
    "sorted_grouped_documents = sorted(grouped_documents.items(), key=lambda x: x[0])\n",
    "\n",
    "# Output the result\n",
    "for file_name, d in sorted_grouped_documents:\n",
    "    print(f\"File: {file_name}\")\n",
    "    for doc in d:\n",
    "        print(f\"  Page: {doc.metadata['page_label']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context=\"\"\n",
    "for i,res in enumerate(sorted_grouped_documents):\n",
    "    print(res[0])\n",
    "    context = f\"{context}Reference number: {i}\\nReference Text:\\n\"\n",
    "    for r in res[1]:\n",
    "        # print(r)\n",
    "        context = f\"{context}\\n{r.page_content}\\n\\n\"\n",
    "    # print(f\"* {res.page_content} [{res.metadata}]\")\n",
    "    # print(\"#\"*25)\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =f\"\"\"\n",
    "\n",
    "Based on the given context provide a comprehensive answer to the following question.\n",
    "If the answer requires listing of something then use bulletpoints or numerical listing.\n",
    "Answer in the same language as the provided question\n",
    "Question: \n",
    "    {question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "                \n",
    "        \"\"\"\n",
    "print(tiktoken_len(prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    prompt\n",
    ")\n",
    "\n",
    "response = llm.complete(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
