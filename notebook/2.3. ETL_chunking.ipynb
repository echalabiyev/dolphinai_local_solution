{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/dolphinai-project/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 30 files: 100%|██████████| 30/30 [00:00<00:00, 145635.56it/s]\n",
      "/data1/dolphinai-project/.venv/lib/python3.12/site-packages/FlagEmbedding/BGE_M3/modeling.py:335: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  colbert_state_dict = torch.load(os.path.join(model_dir, 'colbert_linear.pt'), map_location='cpu')\n",
      "/data1/dolphinai-project/.venv/lib/python3.12/site-packages/FlagEmbedding/BGE_M3/modeling.py:336: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sparse_state_dict = torch.load(os.path.join(model_dir, 'sparse_linear.pt'), map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "from llmsherpa.readers import LayoutPDFReader\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import hashlib\n",
    "from pymilvus.model.hybrid import BGEM3EmbeddingFunction\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "model_id = \"mistralai/Mixtral-8x7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Initialize the embedding function to run on GPU\n",
    "bge_m3_ef = BGEM3EmbeddingFunction(\n",
    "    model_name='BAAI/bge-m3',  # Specify the model name\n",
    "    device='cuda:0',  # Specify the device to use (GPU in this case)\n",
    "    use_fp16=False  # Specify whether to use fp16. Set to `False` if `device` is `cpu`.\n",
    ")\n",
    "\n",
    "llmsherpa_api_url = \"http://localhost:5010/api/parseDocument?renderFormat=all\"\n",
    "pdf_reader = LayoutPDFReader(llmsherpa_api_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_files(folder_path):\n",
    "    # Walk through the directory and its subdirectories\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            # Full path to the file\n",
    "            file_path = os.path.join(root, file)\n",
    "            yield file_path  # Yield file path for further use\n",
    "            \n",
    "# create the length function\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\"\n",
    "    )[\"input_ids\"][0]\n",
    "    return len(tokens)\n",
    "\n",
    "# def add_chunk_to_list(self, chunk_list, uid, idx, main_header, chunk_text, file_name, file_path, metadata, docs_embeddings):\n",
    "def add_chunk_to_list(chunk_list, uid, main_header, chunk_text, file_name, file_path, metadata, docs_embeddings):\n",
    "    \"\"\"Helper function to add the current chunk to the chunk list.\"\"\"\n",
    "    if main_header is not None and chunk_text:\n",
    "        chunk_list.append({\n",
    "            'document_id': uid,\n",
    "            # 'chunk_id': f'{idx}',\n",
    "            'chunk_text': chunk_text,\n",
    "            'chunk_token_length': tiktoken_len(chunk_text),\n",
    "            'file_name': file_name,\n",
    "            'file_path': file_path,\n",
    "            'chunk_name': main_header,\n",
    "            'dense_vector': docs_embeddings['dense'][0],\n",
    "            'sparse_vector': docs_embeddings[\"sparse\"],\n",
    "            'metadata': metadata\n",
    "        })\n",
    "\n",
    "def extract_filename(file_path):\n",
    "    return os.path.basename(file_path)\n",
    "\n",
    "def divide_text_into_pieces(text, num_pieces):\n",
    "    # Calculate the length of each piece\n",
    "    piece_length = len(text) // num_pieces\n",
    "    pieces = [text[i:i+piece_length] for i in range(0, len(text), piece_length)]\n",
    "    \n",
    "    # Ensure the pieces list contains exactly 20 elements\n",
    "    if len(pieces) > num_pieces:\n",
    "        # Merge the last few smaller pieces if necessary\n",
    "        last_piece = ''.join(pieces[num_pieces-1:])\n",
    "        pieces = pieces[:num_pieces-1] + [last_piece]\n",
    "    \n",
    "    return pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting File path: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../artifacts/SAP_files/01.Basic_Function_SD.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunks Processing: 100%|██████████| 1469/1469 [00:19<00:00, 74.84it/s]\n",
      "Getting File path: 1it [00:31, 31.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../artifacts/SAP_files/02.Sales.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunks Processing: 100%|██████████| 2280/2280 [00:33<00:00, 69.00it/s]\n",
      "Getting File path: 2it [01:24, 44.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../artifacts/SAP_files/03.Pricing_Condition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunks Processing: 100%|██████████| 1182/1182 [00:15<00:00, 75.98it/s]\n",
      "Getting File path: 3it [01:47, 34.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../artifacts/SAP_files/04.Availability_Check.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunks Processing: 100%|██████████| 344/344 [00:06<00:00, 54.32it/s]\n",
      "Getting File path: 4it [01:59, 25.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../artifacts/SAP_files/04.McGrawHill-SD.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunks Processing: 100%|██████████| 2045/2045 [01:01<00:00, 33.23it/s]\n",
      "Getting File path: 5it [03:38, 52.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../artifacts/SAP_files/05.Scheduling_Agreement.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunks Processing: 100%|██████████| 1033/1033 [00:14<00:00, 73.51it/s]\n",
      "Getting File path: 6it [04:01, 42.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../artifacts/SAP_files/06.Shipping.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunks Processing: 100%|██████████| 1917/1917 [00:28<00:00, 68.45it/s]\n",
      "Getting File path: 7it [04:51, 44.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../artifacts/SAP_files/07.Transportation.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunks Processing: 100%|██████████| 2338/2338 [00:38<00:00, 61.28it/s]\n",
      "Getting File path: 8it [05:56, 51.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../artifacts/SAP_files/07.Trasportation_46C.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunks Processing: 100%|██████████| 2995/2995 [00:50<00:00, 59.14it/s]\n",
      "Getting File path: 9it [07:16, 60.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../artifacts/SAP_files/08.Billing_process.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunks Processing: 100%|██████████| 1446/1446 [00:17<00:00, 81.80it/s]\n",
      "Getting File path: 10it [07:43, 49.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../artifacts/SAP_files/09.Billing_Plan.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunks Processing: 100%|██████████| 176/176 [00:02<00:00, 76.08it/s]\n",
      "Getting File path: 11it [07:47, 35.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../artifacts/SAP_files/11.Output_Determination.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunks Processing: 100%|██████████| 138/138 [00:02<00:00, 66.04it/s]\n",
      "Getting File path: 12it [07:50, 25.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../artifacts/SAP_files/Credit_Management.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunks Processing: 100%|██████████| 394/394 [00:06<00:00, 65.36it/s]\n",
      "Getting File path: 13it [08:00, 20.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../artifacts/SAP_files/Customer_Service.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunks Processing: 100%|██████████| 405/405 [00:06<00:00, 64.47it/s]\n",
      "Getting File path: 14it [08:09, 17.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../artifacts/SAP_files/Documentary_Payments.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunks Processing: 100%|██████████| 225/225 [00:03<00:00, 66.53it/s]\n",
      "Getting File path: 15it [08:15, 13.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../artifacts/SAP_files/EDI-IDOC_SD.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunks Processing: 100%|██████████| 39/39 [00:00<00:00, 50.75it/s]\n",
      "Getting File path: 16it [08:16, 10.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../artifacts/SAP_files/Foreign Trade.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunks Processing: 100%|██████████| 46/46 [00:00<00:00, 76.60it/s]\n",
      "Getting File path: 17it [08:18,  7.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../artifacts/SAP_files/IACs_Foreign_Trade.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunks Processing: 100%|██████████| 39/39 [00:00<00:00, 82.51it/s]\n",
      "Getting File path: 18it [08:19,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../artifacts/SAP_files/Legal_Control.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunks Processing: 100%|██████████| 271/271 [00:03<00:00, 81.62it/s]\n",
      "Getting File path: 19it [08:23,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../artifacts/SAP_files/Periodic_Declarations_SD-FT-GOV.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunks Processing: 100%|██████████| 420/420 [00:05<00:00, 72.61it/s]\n",
      "Getting File path: 20it [08:37,  7.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../artifacts/SAP_files/Periodic_Declarations.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunks Processing: 100%|██████████| 359/359 [00:05<00:00, 61.84it/s]\n",
      "Getting File path: 21it [08:46,  8.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../artifacts/SAP_files/Preferences.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunks Processing: 100%|██████████| 309/309 [00:04<00:00, 68.39it/s] \n",
      "Getting File path: 22it [08:56,  8.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../artifacts/SAP_files/SCM610_EN_Delivery_Processes.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunks Processing: 100%|██████████| 1563/1563 [00:20<00:00, 75.42it/s]\n",
      "Getting File path: 23it [09:49, 22.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../artifacts/SAP_files/SD-FT-PRO.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunks Processing: 100%|██████████| 335/335 [00:04<00:00, 73.98it/s]\n",
      "Getting File path: 24it [09:57, 17.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../artifacts/SAP_files/Import.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunks Processing: 100%|██████████| 56/56 [00:00<00:00, 78.80it/s]\n",
      "Getting File path: 25it [09:58, 23.92s/it]\n"
     ]
    }
   ],
   "source": [
    "# Initialize hash object for unique ID generation\n",
    "m = hashlib.md5()\n",
    "\n",
    "# folder_path = \"../artifacts/fls/\"\n",
    "folder_path = \"../artifacts/SAP_files/\"\n",
    "files = iterate_files(folder_path)\n",
    "\n",
    "link = \"\"\n",
    "final_list = []\n",
    "for file_path in tqdm(files, desc=\"Getting File path\"):\n",
    "    print(f\"Processing file: {file_path}\")    \n",
    "    # Get File Name\n",
    "    file_name = extract_filename(file_path)    \n",
    "    # Read PDF\n",
    "    doc = pdf_reader.read_pdf(file_path)\n",
    "        \n",
    "    if file_path != link:\n",
    "        m.update(file_path.encode('utf-8'))\n",
    "        uid = m.hexdigest()\n",
    "        link = file_path\n",
    "        # print(uid)\n",
    "        \n",
    "    main_header = None\n",
    "    last_header = None\n",
    "    chunk_text = \"\"\n",
    "    metadata = {}\n",
    "    chunk_list = []\n",
    "    for idx, chunk in enumerate(tqdm(doc.chunks(), desc=\"Chunks Processing\")):\n",
    "        lines = chunk.to_context_text().split(\"\\n\", 1)  # Split at the first newline\n",
    "        header = lines[0].strip()  # The first line is the header\n",
    "        split_headers = [part.strip() for part in header.split('>')]\n",
    "        chunk_bbox = chunk.bbox\n",
    "        chunk_page_idx = chunk.page_idx\n",
    "        \n",
    "        if tiktoken_len(chunk.to_context_text())>1000:\n",
    "            # print(\"tiktoken_len(chunk.to_context_text())>1000\")\n",
    "            pieces = math.ceil(tiktoken_len(chunk.to_context_text()) / 1000)\n",
    "            metadata[header] = {\"page_index\": chunk_page_idx, \"bbox\": chunk_bbox}                \n",
    "            main_header = split_headers[0]                \n",
    "            last_header = split_headers[-1]                                \n",
    "            divided_chunk = divide_text_into_pieces(chunk.to_context_text(), pieces)\n",
    "            for x in divided_chunk:\n",
    "                chunk_text = x\n",
    "                docs_embeddings = bge_m3_ef([chunk_text])\n",
    "                add_chunk_to_list(chunk_list,uid, main_header, chunk_text, file_name, file_path, metadata, docs_embeddings)\n",
    "                # self.add_chunk_to_list(chunk_list,uid, chunk_idx, main_header, chunk_text, file_name, file_path, metadata, docs_embeddings)\n",
    "                # chunk_idx+= 1\n",
    "            metadata = {}\n",
    "        else:\n",
    "            if main_header == split_headers[0] and tiktoken_len(chunk_text + chunk.to_context_text())>1000 : # or len(chunk_text + chunk.to_context_text())>65000:\n",
    "                # print(\"tiktoken_len(chunk_text + chunk.to_context_text())>1000\")\n",
    "                # print(tiktoken_len(chunk_text + chunk.to_context_text()))\n",
    "                # print(tiktoken_len(chunk.to_context_text()))\n",
    "                \n",
    "                # print(split_headers[-1])\n",
    "                docs_embeddings = bge_m3_ef([chunk_text])\n",
    "                # self.add_chunk_to_list(chunk_list,uid, chunk_idx, main_header, chunk_text, file_name, file_path, metadata, docs_embeddings)\n",
    "                add_chunk_to_list(chunk_list,uid, main_header, chunk_text, file_name, file_path, metadata, docs_embeddings)\n",
    "                main_header, last_header, chunk_text = None, None, \"\"\n",
    "\n",
    "            if main_header != split_headers[0] :\n",
    "                if chunk_text:\n",
    "                    docs_embeddings = bge_m3_ef([chunk_text])\n",
    "                    # self.add_chunk_to_list(chunk_list,uid, chunk_idx, main_header, chunk_text, file_name, file_path, metadata, docs_embeddings)\n",
    "                    add_chunk_to_list(chunk_list,uid, main_header, chunk_text, file_name, file_path, metadata, docs_embeddings)\n",
    "                metadata = {}\n",
    "                main_header = split_headers[0]\n",
    "                chunk_text = chunk.to_context_text()\n",
    "                last_header = split_headers[-1]\n",
    "            elif last_header != split_headers[-1]:\n",
    "                # When the last header changes but the main header remains the same\n",
    "                last_header = split_headers[-1]\n",
    "                chunk_text += \"\\n\\n\" + split_headers[-1] + \"\\n\" + lines[1] + \"\\n\"\n",
    "            else:\n",
    "                # If it's the same main header and last header, append the content\n",
    "                chunk_text += lines[1]\n",
    "            metadata[header] = {\"page_index\":chunk_page_idx,\"bbox\":chunk_bbox}\n",
    "        \n",
    "    # # Add the last chunk after the loop ends\n",
    "    # docs_embeddings = bge_m3_ef([chunk_text])\n",
    "    # add_chunk_to_list(chunk_list,uid, main_header, chunk_text, file_name, file_path, metadata, docs_embeddings)\n",
    "    if chunk_text:\n",
    "        docs_embeddings = bge_m3_ef([chunk_text])\n",
    "        add_chunk_to_list(chunk_list, uid, main_header, chunk_text, file_name, file_path, metadata, docs_embeddings)\n",
    "        # self.add_chunk_to_list(chunk_list, uid, chunk_idx, main_header, chunk_text, file_name, file_path, metadata, docs_embeddings)\n",
    "        \n",
    "    # Convert the chunk_list to a Pandas DataFrame\n",
    "    chunk_df = pd.DataFrame(chunk_list)\n",
    "    # chunk_df.sort_values(by='chunk_id', ascending=False)\n",
    "    chunk_df['chunk_id'] = chunk_df.index\n",
    "    # Convert DataFrame to a list of dictionaries\n",
    "    chunk_list = chunk_df.to_dict(orient='records')\n",
    "    final_list.extend(chunk_list)\n",
    "    # chunk_list =[]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2210"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_list_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "# 1. Set up a Milvus client\n",
    "client = MilvusClient(\n",
    "    uri=\"http://localhost:19530/dolphinai_db\"\n",
    ")\n",
    "client.list_collections() \n",
    "\n",
    "# Load a collection\n",
    "client.load_collection(\n",
    "    # collection_name=\"dolphinai_collection\",\n",
    "    collection_name=\"hybrid_sap_collection\",\n",
    "    replica_number=1 # Number of replicas to create on query nodes. Max value is 1 for Milvus Standalone, and no greater than `queryNode.replicas` for Milvus Cluster.\n",
    ")\n",
    "\n",
    "res = client.get_load_state(\n",
    "    collection_name=\"hybrid_sap_collection\"\n",
    "    # collection_name=\"dolphinai_collection\"\n",
    ")\n",
    "\n",
    "print(res)\n",
    "\n",
    "res = client.insert(\n",
    "    collection_name=\"hybrid_sap_collection\",\n",
    "    # collection_name=\"dolphinai_collection\",\n",
    "    data=final_list\n",
    ")\n",
    "\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
